const App = require("./app");
const config = require("./config/env");
const database = require("./config/db");

/**
 * OpenLLM Monitor Server
 */
class Server {
  constructor() {
    this.app = new App();
    this.port = config.port;
  }

  /**
   * Start the server
   */
  async start() {
    try {
      // Connect to database
      console.log("ðŸ”Œ Connecting to database...");
      await database.connect();

      // Start HTTP server
      this.server = this.app.getServer();

      this.server.listen(this.port, () => {
        console.log(`
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                           OpenLLM Monitor Server                             â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ðŸš€ Server running on port ${this.port.toString().padEnd(52)}                â•‘
â•‘  ðŸŒ Environment: ${config.nodeEnv.toUpperCase().padEnd(59)}                  â•‘
â•‘  ðŸ“Š Database: ${database.getConnectionStatus().padEnd(62)}                   â•‘
â•‘  ðŸ”— API Base URL: http://localhost:${this.port}/api${" ".repeat(37)}         â•‘
â•‘  ðŸ“¡ WebSocket: http://localhost:${this.port}${" ".repeat(42)}                â•‘
â•‘  ðŸ“š API Info: http://localhost:${this.port}/api/info${" ".repeat(35)}        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸŽ¯ Ready to monitor LLM requests!

Supported Providers:
  â€¢ OpenAI (GPT-3.5, GPT-4)
  â€¢ OpenRouter (Multi-model access)
  â€¢ Mistral AI (Mistral models)
  â€¢ Ollama (Local models)

Features:
  âœ… Real-time request logging
  âœ… Prompt replay & comparison
  âœ… Cost tracking & analysis
  âœ… Performance monitoring
  âœ… Error analysis & debugging
  âœ… WebSocket real-time updates

Environment Configuration:
  â€¢ Node.js: ${process.version}
  â€¢ MongoDB: ${database.mongoUri}
  â€¢ CORS Origins: ${config.corsOrigins.join(", ")}
  â€¢ Rate Limiting: ${config.rateLimit.maxRequests} requests per ${
          config.rateLimit.windowMs / 1000 / 60
        } minutes

ðŸ”§ To configure providers, visit: http://localhost:${this.port}/api/providers
ðŸ“– For API documentation, visit: http://localhost:${this.port}/api/info
        `);

        this.displayProviderStatus();
      });

      // Handle server errors
      this.server.on("error", (error) => {
        if (error.code === "EADDRINUSE") {
          console.error(`âŒ Port ${this.port} is already in use`);
          process.exit(1);
        } else {
          console.error("âŒ Server error:", error);
        }
      });
    } catch (error) {
      console.error("âŒ Failed to start server:", error);
      process.exit(1);
    }
  }

  /**
   * Display provider status on startup
   */
  async displayProviderStatus() {
    try {
      const providers = {
        OpenAI: config.providers.openai.apiKey
          ? "ðŸŸ¢ API Key Set"
          : "ðŸ”´ No API Key",
        OpenRouter: config.providers.openrouter.apiKey
          ? "ðŸŸ¢ API Key Set"
          : "ðŸ”´ No API Key",
        Mistral: config.providers.mistral.apiKey
          ? "ðŸŸ¢ API Key Set"
          : "ðŸ”´ No API Key",
        Ollama: "ðŸŸ¡ Local (No API Key Required)",
      };

      console.log("\nðŸ“¡ Provider Status:");
      Object.entries(providers).forEach(([name, status]) => {
        console.log(`  ${name}: ${status}`);
      });

      // Test connections in background
      setTimeout(() => {
        this.testProviderConnections();
      }, 2000);
    } catch (error) {
      console.error("Error displaying provider status:", error);
    }
  }

  /**
   * Test provider connections
   */
  async testProviderConnections() {
    const services = {
      OpenAI: () => new (require("./services/openaiService"))(),
      OpenRouter: () => new (require("./services/openrouterService"))(),
      Ollama: () => new (require("./services/ollamaService"))(),
    };

    console.log("\nðŸ” Testing provider connections...");

    for (const [name, createService] of Object.entries(services)) {
      try {
        const service = createService();
        const startTime = Date.now();
        const isConnected = await service.testConnection();
        const latency = Date.now() - startTime;

        if (isConnected) {
          console.log(`  ${name}: ðŸŸ¢ Connected (${latency}ms)`);
        } else {
          console.log(`  ${name}: ðŸ”´ Connection failed`);
        }
      } catch (error) {
        console.log(`  ${name}: ðŸ”´ Error - ${error.message}`);
      }
    }

    console.log("\nðŸŽ‰ Server initialization complete!\n");
  }

  /**
   * Stop the server
   */
  async stop() {
    if (this.server) {
      return new Promise((resolve) => {
        this.server.close(() => {
          console.log("ðŸ‘‹ Server stopped");
          resolve();
        });
      });
    }
  }
}

// Start server if this file is run directly
if (require.main === module) {
  const server = new Server();
  server.start().catch((error) => {
    console.error("Failed to start server:", error);
    process.exit(1);
  });
}

module.exports = Server;
